name: "fio-benchmark"

on:
  schedule:
    - cron:  '0 0 * * *'
  workflow_dispatch:

jobs:
  fio-benchmark:
    runs-on: [self-hosted, daily-build, bench-01]
    steps:
      - name: Checkout
        uses: actions/checkout@v2
        with:
          fetch-depth: 1

      - name: Set Variable
        id: vars
        run: |
          echo ::set-output name=META_URL::redis://mymaster,172.27.0.1,172.27.0.2,172.27.0.3:26379/6
          echo ::set-output name=MINIO_URL::http://172.27.0.2:9005/juicefs-fio-benchmark1
          echo ::set-output name=MOUNT_POINT::/tmp/juicefs-fio-benchmark

      - name: Build linux target
        run: |
          export GOPATH=/usr/local/go
          export HOME=/root
          make juicefs

      - name: Clean Before
        run: |
          if [ -d ${{ steps.vars.outputs.MOUNT_POINT }} ]; then
            ./juicefs umount ${{ steps.vars.outputs.MOUNT_POINT }} || true
          fi
          UUID=$(./juicefs status ${{ steps.vars.outputs.META_URL }} | grep UUID | cut -d '"' -f 4)
          if [ -n "$UUID" ];then
            sudo ./juicefs destroy --force ${{ steps.vars.outputs.META_URL }} $UUID
          fi

      - name: Format
        run: |
          sudo ./juicefs format --storage minio --bucket  ${{ steps.vars.outputs.MINIO_URL }} --access-key minioadmin --secret-key ${{ secrets.MINIO_SECRET_KEY }}  ${{ steps.vars.outputs.META_URL }} fio-benchmark

      - name: Fio Benchmark 
        run: |
          csv_file=/var/lib/mysql-files/result.csv
          juicefs_version=$(./juicefs -V|cut -b 17-)
          echo bandwidth, created_date, github_revision, workflow_url, juicefs_version, name, ref_name > $csv_file
          default_size=1G
          fio_jobs=(
            "big-file-sequential-read:      --rw=read --refill_buffers --bs=256k --size=${default_size}" \
            "big-file-sequential-write:     --rw=write --refill_buffers --bs=256k --size=${default_size}" \
            "big-file-multi-read-1:         --rw=read --refill_buffers --bs=256k --size=${default_size} --numjobs=1" \
            "big-file-multi-read-2:         --rw=read --refill_buffers --bs=256k --size=${default_size} --numjobs=2" \
            "big-file-multi-read-4:         --rw=read --refill_buffers --bs=256k --size=${default_size} --numjobs=4" \
            "big-file-multi-read-8:         --rw=read --refill_buffers --bs=256k --size=${default_size} --numjobs=8" \
            "big-file-multi-read-16:        --rw=read --refill_buffers --bs=256k --size=${default_size} --numjobs=16" \
            "big-file-multi-write-1:        --rw=write --refill_buffers --bs=256k --size=${default_size} --numjobs=1" \
            "big-file-multi-write-2:        --rw=write --refill_buffers --bs=256k --size=${default_size} --numjobs=2" \
            "big-file-multi-write-4:        --rw=write --refill_buffers --bs=256k --size=${default_size} --numjobs=4" \
            "big-file-multi-write-8:        --rw=write --refill_buffers --bs=256k --size=${default_size} --numjobs=8" \
            "big-file-multi-write-16:       --rw=write --refill_buffers --bs=256k --size=${default_size} --numjobs=16" \
            "big-file-rand-read-4k:         --rw=randread --refill_buffers --size=${default_size} --filename=randread.bin --bs=4k" \
            "big-file-rand-read-16k:        --rw=randread --refill_buffers --size=${default_size} --filename=randread.bin --bs=16k" \
            "big-file-rand-read-64k:        --rw=randread --refill_buffers --size=${default_size} --filename=randread.bin --bs=64k" \
            "big-file-rand-read-256k:       --rw=randread --refill_buffers --size=${default_size} --filename=randread.bin --bs=256k" \
            "big-file-random-write-4k:      --rw=randwrite --refill_buffers --size=${default_size} --bs=4k" \
            "big-file-random-write-16k:     --rw=randwrite --refill_buffers --size=${default_size} --bs=16k" \
            "big-file-random-write-64k:     --rw=randwrite --refill_buffers --size=${default_size} --bs=64k" \
            "big-file-random-write-256k:    --rw=randwrite --refill_buffers --size=${default_size} --bs=256k" \
            "small-file-seq-read-4k:        --rw=read --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=1000  :--cache-size=0" \
            "small-file-seq-read-16k:       --rw=read --file_service_type=sequential --bs=16k --filesize=16k --nrfiles=1000 :--cache-size=0" \
            "small-file-seq-read-64k:       --rw=read --file_service_type=sequential --bs=64k --filesize=64k --nrfiles=1000 :--cache-size=0" \
            "small-file-seq-read-128k:      --rw=read --file_service_type=sequential --bs=128k --filesize=128k --nrfiles=1000 :--cache-size=0" \
            "small-file-seq-read-256k:      --rw=read --file_service_type=sequential --bs=256k --filesize=256k --nrfiles=1000 :--cache-size=0" \
            "small-file-seq-write-4k:       --rw=write --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=1000 :--writeback" \
            "small-file-seq-write-16k:      --rw=write --file_service_type=sequential --bs=16k --filesize=16k --nrfiles=1000 :--writeback" \
            "small-file-seq-write-64k:      --rw=write --file_service_type=sequential --bs=64k --filesize=64k --nrfiles=1000 :--writeback" \
            "small-file-seq-write-128k:     --rw=write --file_service_type=sequential --bs=128k --filesize=128k --nrfiles=1000 :--writeback" \
            "small-file-seq-write-256k:     --rw=write --file_service_type=sequential --bs=256k --filesize=256k --nrfiles=1000 :--writeback" \
            "small-file-multi-read-1:       --rw=read --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=1000 --numjobs=1" \
            "small-file-multi-read-2:       --rw=read --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=1000 --numjobs=2" \
            "small-file-multi-read-4:       --rw=read --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=1000 --numjobs=4" \
            "small-file-multi-read-8:       --rw=read --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=1000 --numjobs=8" \
            "small-file-multi-read-16:      --rw=read --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=1000 --numjobs=16" \
            "small-file-multi-write-1:      --rw=write --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=1000 --numjobs=1" \
            "small-file-multi-write-2:      --rw=write --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=1000 --numjobs=2" \
            "small-file-multi-write-4:      --rw=write --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=1000 --numjobs=4" \
            "small-file-multi-write-8:      --rw=write --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=1000 --numjobs=8" \
            "small-file-multi-write-16:     --rw=write --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=1000 --numjobs=16" \
          )

          for job in "${fio_jobs[@]}"
          do
            echo job is:, $job
            if [ -n "$job" ]; then
              name=$(echo $job | awk -F: '{print $1}' | xargs)
              fio_arg=$(echo $job | awk -F: '{print $2}' | xargs)
              mount_arg=$(echo $job | awk -F: '{print $3}' | xargs)
              sudo ./juicefs mount -d ${{ steps.vars.outputs.META_URL }} ${{ steps.vars.outputs.MOUNT_POINT }} --no-usage-report $mount_arg &
              if [[ "$name" =~ ^big-file-rand-read.* ]]; then
                block_size=$(echo $name | awk -F- '{print $NF}' | xargs)
                echo block_size is $block_size
                fio --name=big-file-rand-read-preload --directory=${{ steps.vars.outputs.MOUNT_POINT }} --rw=randread --refill_buffers --size=${default_size} --filename=randread.bin --bs=$block_size --pre_read=1
                sudo sync && echo 3 > /proc/sys/vm/drop_caches
              fi
              fio --name=$name --directory=${{ steps.vars.outputs.MOUNT_POINT }} $fio_arg > result.log
              bw=$(tail -1 result.log | awk '{print $2}' | awk -F '=' '{print $2}' | cut -b 1,-3)
              echo $bw, $(date +"%Y-%m-%d %H:%M:%S"), ${{github.sha}}, https://github.com/${{github.repository}}/actions/runs/${{github.run_id}}, $juicefs_version, $name, ${{ github.ref_name }} | tee -a $csv_file
              sudo ./juicefs umount ${{ steps.vars.outputs.MOUNT_POINT }}
            fi
          done
          mysql -h 172.27.0.1 -u root -p${{secrets.MYSQL_PASSWORD}} -e "use  test_result; LOAD DATA INFILE '/var/lib/mysql-files/result.csv' INTO TABLE fio_test FIELDS TERMINATED BY ',' IGNORE 1 ROWS;" 

      - name: Clean After
        run: |
          if [ -d ${{ steps.vars.outputs.MOUNT_POINT }} ]; then
            ./juicefs umount ${{ steps.vars.outputs.MOUNT_POINT }} || true
          fi
          UUID=$(./juicefs status ${{ steps.vars.outputs.META_URL }} | grep UUID | cut -d '"' -f 4)
          if [ -n "$UUID" ];then
            sudo ./juicefs destroy --force ${{ steps.vars.outputs.META_URL }} $UUID
          fi

#      - name: Send Slack Notification
#        if: ${{ failure() }}
#        id: slack
#        uses: slackapi/slack-github-action@v1.18.0
#        with:
#          channel-id: "${{ secrets.SLACK_CHANNEL_ID_FOR_PR_CHECK_NOTIFY }}"
#          slack-message: "@${{ github.actor}} CI Check ${{ job.status }} on ${{ github.event_name }}\n${{ github.event.pull_request.title || github.event.head_commit.message }} \nhttps://github.com/${{github.repository}}/actions/runs/${{github.run_id}}"
#        env:
#          SLACK_BOT_TOKEN: ${{ secrets.SLACK_BOT_TOKEN }}  