name: "fio-benchmark"

on:
  schedule:
    - cron:  '0 0 * * *'
  workflow_dispatch:

jobs:
  fio-benchmark:
    if: github.repository == 'juicedata/juicefs'
    runs-on: [self-hosted, daily-build, bench-01]
    steps:
      - name: Checkout
        uses: actions/checkout@v2
        with:
          fetch-depth: 1

      - name: Set Variable
        id: vars
        run: |
          echo ::set-output name=META_URL::redis://mymaster,172.27.0.1,172.27.0.2,172.27.0.3:26379/6
          echo ::set-output name=MOUNT_POINT::/tmp/juicefs-fio-benchmark
          echo ::set-output name=BACKWARD_VERSIONS::1

      - name: Build linux target
        run: |
          export GOPATH=/usr/local/go
          export HOME=/root
          make juicefs

      - name: Clean Before
        run: |
          rm /var/jfsCache/ -rf
          if [ -d ${{ steps.vars.outputs.MOUNT_POINT }} ]; then
            ./juicefs umount ${{ steps.vars.outputs.MOUNT_POINT }} || true
          fi
          UUID=$(./juicefs status ${{ steps.vars.outputs.META_URL }} | grep UUID | cut -d '"' -f 4)
          if [ -n "$UUID" ];then
            sudo ./juicefs destroy --force ${{ steps.vars.outputs.META_URL }} $UUID
          fi

      - name: Fio Benchmark 
        run: |
          default_size=1G
          warning_value=90
          fio_jobs=(
            "big-file-sequential-read:      --rw=read --refill_buffers --bs=256k --size=${default_size}" \
            "big-file-sequential-write:     --rw=write --refill_buffers --bs=256k --size=${default_size}" \
            "big-file-multi-read-1:         --rw=read --refill_buffers --bs=256k --size=${default_size} --numjobs=1" \
            "big-file-multi-read-2:         --rw=read --refill_buffers --bs=256k --size=${default_size} --numjobs=2" \
            "big-file-multi-read-4:         --rw=read --refill_buffers --bs=256k --size=${default_size} --numjobs=4" \
            "big-file-multi-read-8:         --rw=read --refill_buffers --bs=256k --size=${default_size} --numjobs=8" \
            "big-file-multi-read-16:        --rw=read --refill_buffers --bs=256k --size=${default_size} --numjobs=16" \
            "big-file-multi-write-1:        --rw=write --refill_buffers --bs=256k --size=${default_size} --numjobs=1" \
            "big-file-multi-write-2:        --rw=write --refill_buffers --bs=256k --size=${default_size} --numjobs=2" \
            "big-file-multi-write-4:        --rw=write --refill_buffers --bs=256k --size=${default_size} --numjobs=4" \
            "big-file-multi-write-8:        --rw=write --refill_buffers --bs=256k --size=${default_size} --numjobs=8" \
            "big-file-multi-write-16:       --rw=write --refill_buffers --bs=256k --size=${default_size} --numjobs=16" \
            "big-file-rand-read-4k:         --rw=randread --refill_buffers --size=${default_size} --filename=randread.bin --bs=4k" \
            "big-file-rand-read-16k:        --rw=randread --refill_buffers --size=${default_size} --filename=randread.bin --bs=16k" \
            "big-file-rand-read-64k:        --rw=randread --refill_buffers --size=${default_size} --filename=randread.bin --bs=64k" \
            "big-file-rand-read-256k:       --rw=randread --refill_buffers --size=${default_size} --filename=randread.bin --bs=256k" \
            "big-file-random-write-4k:      --rw=randwrite --refill_buffers --size=${default_size} --bs=4k" \
            "big-file-random-write-16k:     --rw=randwrite --refill_buffers --size=${default_size} --bs=16k" \
            "big-file-random-write-64k:     --rw=randwrite --refill_buffers --size=${default_size} --bs=64k" \
            "big-file-random-write-256k:    --rw=randwrite --refill_buffers --size=${default_size} --bs=256k" \
            "small-file-seq-read-4k:        --rw=read --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=1000  :--cache-size=0" \
            "small-file-seq-read-16k:       --rw=read --file_service_type=sequential --bs=16k --filesize=16k --nrfiles=1000 :--cache-size=0" \
            "small-file-seq-read-64k:       --rw=read --file_service_type=sequential --bs=64k --filesize=64k --nrfiles=1000 :--cache-size=0" \
            "small-file-seq-read-128k:      --rw=read --file_service_type=sequential --bs=128k --filesize=128k --nrfiles=1000 :--cache-size=0" \
            "small-file-seq-read-256k:      --rw=read --file_service_type=sequential --bs=256k --filesize=256k --nrfiles=1000 :--cache-size=0" \
            "small-file-seq-write-4k:       --rw=write --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=1000 :--writeback" \
            "small-file-seq-write-16k:      --rw=write --file_service_type=sequential --bs=16k --filesize=16k --nrfiles=1000 :--writeback" \
            "small-file-seq-write-64k:      --rw=write --file_service_type=sequential --bs=64k --filesize=64k --nrfiles=1000 :--writeback" \
            "small-file-seq-write-128k:     --rw=write --file_service_type=sequential --bs=128k --filesize=128k --nrfiles=1000 :--writeback" \
            "small-file-seq-write-256k:     --rw=write --file_service_type=sequential --bs=256k --filesize=256k --nrfiles=1000 :--writeback" \
            "small-file-multi-read-1:       --rw=read --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=1000 --numjobs=1" \
            "small-file-multi-read-2:       --rw=read --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=1000 --numjobs=2" \
            "small-file-multi-read-4:       --rw=read --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=1000 --numjobs=4" \
            "small-file-multi-read-8:       --rw=read --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=1000 --numjobs=8" \
            "small-file-multi-read-16:      --rw=read --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=1000 --numjobs=16" \
            "small-file-multi-write-1:      --rw=write --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=1000 --numjobs=1" \
            "small-file-multi-write-2:      --rw=write --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=1000 --numjobs=2" \
            "small-file-multi-write-4:      --rw=write --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=1000 --numjobs=4" \
            "small-file-multi-write-8:      --rw=write --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=1000 --numjobs=8" \
            "small-file-multi-write-16:     --rw=write --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=1000 --numjobs=16" \
          )
          urls=("juicefs")
          urls+=($(curl -s https://api.github.com/repos/juicedata/juicefs/releases | grep browser_download_url | grep linux-amd64.tar.gz | awk -F\" '{print $4}' | head -${{ steps.vars.outputs.BACKWARD_VERSIONS }}))          
          for url in "${urls[@]}"; do
            csv_file=/var/lib/mysql-files/result.csv
            test -f $csv_file  && rm $csv_file -f
            if [[ $url == http* ]]; then  
              rm juicefs 
              echo download url is: $url
              wget -q $url
              tar -zxf $(basename $url)
              rm $(basename $url)
            fi
            juicefs_version=$(./juicefs -V|cut -b 17-)
            echo juicefs version: $(./juicefs -V)
            for job in "${fio_jobs[@]}"
            do
              echo job is:, $job
              if [ -n "$job" ]; then
                set -x
                name=$(echo $job | awk -F: '{print $1}' | xargs)
                fio_arg=$(echo $job | awk -F: '{print $2}' | xargs)
                mount_arg=$(echo $job | awk -F: '{print $3}' | xargs)
                minio_url=http://172.27.0.2:9005/$name
                mc ls myminio/$name && mc rb  myminio/$name --force --dangerous || printf "Warining; remove bucket failed: %s, exit code: %s\n" "myminio/$name" "$?"
                rm /var/jfsCache/ -rf
                while true; do
                  used=$(df -h /mnt/minio/data1  | awk 'NR>1 {gsub(/%/,""); print $5}') 
                  if [ $used -gt $warning_value ]; then
                    printf "disk used %s%%, sleep 10s\n" "$used"
                    sleep 10s
                  else
                    break
                  fi
                done
                ./juicefs format --trash-days 0 --storage minio --bucket  $minio_url --access-key minioadmin --secret-key ${{ secrets.MINIO_SECRET_KEY }}  ${{ steps.vars.outputs.META_URL }} fio-benchmark
                ./juicefs mount -d ${{ steps.vars.outputs.META_URL }} ${{ steps.vars.outputs.MOUNT_POINT }} --no-usage-report $mount_arg &
                if [[ "$name" =~ ^big-file-rand-read.* ]]; then
                  block_size=$(echo $name | awk -F- '{print $NF}' | xargs)
                  echo block_size is $block_size
                  fio --name=big-file-rand-read-preload --directory=${{ steps.vars.outputs.MOUNT_POINT }} --rw=randread --refill_buffers --size=${default_size} --filename=randread.bin --bs=$block_size --pre_read=1
                  sudo sync && echo 3 > /proc/sys/vm/drop_caches
                fi
                fio --name=$name --directory=${{ steps.vars.outputs.MOUNT_POINT }} $fio_arg > result.log
                bw=$(tail -1 result.log | awk '{print $2}' | awk -F '=' '{print $2}' | cut -b 1,-3)
                echo $bw, $(date +"%Y-%m-%d %H:%M:%S"), ${{github.sha}}, https://github.com/${{github.repository}}/actions/runs/${{github.run_id}}, $juicefs_version, $name, ${{ github.ref_name }}, $default_size, redis, minio | tee -a $csv_file
                
                ./juicefs umount -f ${{ steps.vars.outputs.MOUNT_POINT }}
                UUID=$(./juicefs status ${{ steps.vars.outputs.META_URL }} | grep UUID | cut -d '"' -f 4)
                if [ -n "$UUID" ];then
                  sudo ./juicefs destroy --force ${{ steps.vars.outputs.META_URL }} $UUID
                fi
                mc rb  myminio/$name --force --dangerous || printf "Warining; remove bucket failed: %s, exit code: %s" "myminio/$name" "$?"
                set +x
              fi
            done
            mysql -h 172.27.0.1 -u root -p${{secrets.MYSQL_PASSWORD}} -e "use  test_result; LOAD DATA INFILE '/var/lib/mysql-files/result.csv' INTO TABLE fio_test FIELDS TERMINATED BY ',' (bandwidth, created_date, github_revision, workflow_url, juicefs_version, name, ref_name, size, meta_engine, storage); " 
          done
          

      - name: Clean After
        run: |
          if [ -d ${{ steps.vars.outputs.MOUNT_POINT }} ]; then
            ./juicefs umount ${{ steps.vars.outputs.MOUNT_POINT }} || true
          fi
          UUID=$(./juicefs status ${{ steps.vars.outputs.META_URL }} | grep UUID | cut -d '"' -f 4)
          if [ -n "$UUID" ];then
            sudo ./juicefs destroy --force ${{ steps.vars.outputs.META_URL }} $UUID
          fi

      - name: Send Slack Notification
        if: ${{ failure() }}
        uses: juicedata/slack-notify-action@main
        with:
          channel-id: "${{ secrets.SLACK_CHANNEL_ID_FOR_PR_CHECK_NOTIFY }}"
          slack_bot_token: "${{ secrets.SLACK_BOT_TOKEN }}"  
