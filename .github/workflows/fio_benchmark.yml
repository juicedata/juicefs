name: "fio-benchmark"

on:
  schedule:
    - cron:  '0 0 * * *'
  workflow_dispatch:
    inputs:
      size:
        description: 'file size to read/write by fio'
        required: true
        default: '1G'
      nrfiles:
        description: 'number of files to read/write by fio'
        required: true
        default: 10000
jobs:
  fio-benchmark:
    if: github.repository == 'juicedata/juicefs'
    runs-on: [self-hosted, daily-build, bench-01]
    steps:
      - name: Checkout
        uses: actions/checkout@v2
        with:
          fetch-depth: 1

      - name: Set Variable
        id: vars
        run: |
          echo ::set-output name=META_URL::redis://mymaster,172.27.0.1,172.27.0.2,172.27.0.3:26379/6
          echo ::set-output name=MOUNT_POINT::/tmp/juicefs-fio-benchmark
          echo ::set-output name=BACKWARD_VERSIONS::1

      - name: Build linux target
        run: |
          export GOPATH=/usr/local/go
          export HOME=/root
          make juicefs

      - name: Clean Before
        run: |
          rm /var/jfsCache/ -rf
          if [ -d ${{ steps.vars.outputs.MOUNT_POINT }} ]; then
            ./juicefs umount ${{ steps.vars.outputs.MOUNT_POINT }} || true
          fi
          UUID=$(./juicefs status ${{ steps.vars.outputs.META_URL }} | grep UUID | cut -d '"' -f 4)
          if [ -n "$UUID" ];then
            sudo ./juicefs destroy --force ${{ steps.vars.outputs.META_URL }} $UUID
          fi

      - name: Fio Benchmark 
        run: |
          size=${{ github.event.inputs.size }}
          nrfiles=${{ github.event.inputs.nrfiles }}
          disk_warning_value=90
          decrease_ratio=0.8
          fio_jobs=(
            "big-file-sequential-read:      --rw=read --refill_buffers --bs=256k --size=${size}" \
            "big-file-sequential-write:     --rw=write --refill_buffers --bs=256k --size=${size}" \
            "big-file-multi-read-1:         --rw=read --refill_buffers --bs=256k --size=${size} --numjobs=1" \
            "big-file-multi-read-2:         --rw=read --refill_buffers --bs=256k --size=${size} --numjobs=2" \
            "big-file-multi-read-4:         --rw=read --refill_buffers --bs=256k --size=${size} --numjobs=4" \
            "big-file-multi-read-8:         --rw=read --refill_buffers --bs=256k --size=${size} --numjobs=8" \
            "big-file-multi-read-16:        --rw=read --refill_buffers --bs=256k --size=${size} --numjobs=16" \
            "big-file-multi-write-1:        --rw=write --refill_buffers --bs=256k --size=${size} --numjobs=1" \
            "big-file-multi-write-2:        --rw=write --refill_buffers --bs=256k --size=${size} --numjobs=2" \
            "big-file-multi-write-4:        --rw=write --refill_buffers --bs=256k --size=${size} --numjobs=4" \
            "big-file-multi-write-8:        --rw=write --refill_buffers --bs=256k --size=${size} --numjobs=8" \
            "big-file-multi-write-16:       --rw=write --refill_buffers --bs=256k --size=${size} --numjobs=16" \
            "big-file-rand-read-4k:         --rw=randread --refill_buffers --size=${size} --filename=randread.bin --bs=4k" \
            "big-file-rand-read-16k:        --rw=randread --refill_buffers --size=${size} --filename=randread.bin --bs=16k" \
            "big-file-rand-read-64k:        --rw=randread --refill_buffers --size=${size} --filename=randread.bin --bs=64k" \
            "big-file-rand-read-256k:       --rw=randread --refill_buffers --size=${size} --filename=randread.bin --bs=256k" \
            "big-file-random-write-4k:      --rw=randwrite --refill_buffers --size=${size} --bs=4k" \
            "big-file-random-write-16k:     --rw=randwrite --refill_buffers --size=${size} --bs=16k" \
            "big-file-random-write-64k:     --rw=randwrite --refill_buffers --size=${size} --bs=64k" \
            "big-file-random-write-256k:    --rw=randwrite --refill_buffers --size=${size} --bs=256k" \
            "small-file-seq-read-4k:        --rw=read --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=${nrfiles}  :--cache-size=0" \
            "small-file-seq-read-16k:       --rw=read --file_service_type=sequential --bs=16k --filesize=16k --nrfiles=${nrfiles} :--cache-size=0" \
            "small-file-seq-read-64k:       --rw=read --file_service_type=sequential --bs=64k --filesize=64k --nrfiles=${nrfiles} :--cache-size=0" \
            "small-file-seq-read-128k:      --rw=read --file_service_type=sequential --bs=128k --filesize=128k --nrfiles=${nrfiles} :--cache-size=0" \
            "small-file-seq-read-256k:      --rw=read --file_service_type=sequential --bs=256k --filesize=256k --nrfiles=${nrfiles} :--cache-size=0" \
            "small-file-seq-write-4k:       --rw=write --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=${nrfiles} :--writeback" \
            "small-file-seq-write-16k:      --rw=write --file_service_type=sequential --bs=16k --filesize=16k --nrfiles=${nrfiles} :--writeback" \
            "small-file-seq-write-64k:      --rw=write --file_service_type=sequential --bs=64k --filesize=64k --nrfiles=${nrfiles} :--writeback" \
            "small-file-seq-write-128k:     --rw=write --file_service_type=sequential --bs=128k --filesize=128k --nrfiles=${nrfiles} :--writeback" \
            "small-file-seq-write-256k:     --rw=write --file_service_type=sequential --bs=256k --filesize=256k --nrfiles=${nrfiles} :--writeback" \
            "small-file-multi-read-1:       --rw=read --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=${nrfiles} --numjobs=1" \
            "small-file-multi-read-2:       --rw=read --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=${nrfiles} --numjobs=2" \
            "small-file-multi-read-4:       --rw=read --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=${nrfiles} --numjobs=4" \
            "small-file-multi-read-8:       --rw=read --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=${nrfiles} --numjobs=8" \
            "small-file-multi-read-16:      --rw=read --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=${nrfiles} --numjobs=16" \
            "small-file-multi-write-1:      --rw=write --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=${nrfiles} --numjobs=1" \
            "small-file-multi-write-2:      --rw=write --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=${nrfiles} --numjobs=2" \
            "small-file-multi-write-4:      --rw=write --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=${nrfiles} --numjobs=4" \
            "small-file-multi-write-8:      --rw=write --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=${nrfiles} --numjobs=8" \
            "small-file-multi-write-16:     --rw=write --file_service_type=sequential --bs=4k --filesize=4k --nrfiles=${nrfiles} --numjobs=16" \
          )
          csv_file=/var/lib/mysql-files/result.csv
          test -f $csv_file  && rm $csv_file -f
          urls=("juicefs")
          urls+=($(curl -s https://api.github.com/repos/juicedata/juicefs/releases | grep browser_download_url | grep linux-amd64.tar.gz | awk -F\" '{print $4}' | head -${{ steps.vars.outputs.BACKWARD_VERSIONS }}))          
          for url in "${urls[@]}"; do
            if [[ $url == http* ]]; then  
              echo download url is: $url
              wget -q $url
              bin_name=$(echo $(basename $url) | sed 's/.tar.gz//g')
              mkdir jfs || true
              tar -zxf $(basename $url) -C jfs
              mv jfs/juicefs $bin_name
              rm $(basename $url)
              rm jfs -rf
            fi
          done

          fail=false
          mkdir fio || true
          for job in "${fio_jobs[@]}"; do
            echo job is:: $job
            bw_list=()
            for url in "${urls[@]}"; do
              echo url is: $url
              bin_name=$(echo $(basename $url) | sed 's/.tar.gz//g') 
              juicefs_version=$(./$bin_name -V|cut -b 17-)
              echo juicefs version: $(./$bin_name -V)
              if [ -n "$job" ]; then
                name=$(echo $job | awk -F: '{print $1}' | xargs)
                fio_arg=$(echo $job | awk -F: '{print $2}' | xargs)
                mount_arg=$(echo $job | awk -F: '{print $3}' | xargs)
                minio_url=http://172.27.0.2:9005/$name
                mc ls myminio/$name && mc rb  myminio/$name --force --dangerous || printf "Warining; remove bucket failed: %s, exit code: %s\n" "myminio/$name" "$?"
                rm /var/jfsCache/ -rf
                while true; do
                  used=$(df -h /mnt/minio/data1  | awk 'NR>1 {gsub(/%/,""); print $5}') 
                  if [ $used -gt $disk_warning_value ]; then
                    printf "disk used %s%%, sleep 10s\n" "$used"
                    sleep 10s
                  else
                    break
                  fi
                done
                rev=$(./$bin_name -V | awk -F- '{print $1}' | awk -F ' ' '{print $3}')
                if [[ "$rev" < "1.0.0" ]]; then
                  trash_day=""
                else
                  trash_day="--trash-days 0"
                fi
                ./$bin_name format $trash_day --storage minio --bucket  $minio_url --access-key minioadmin --secret-key ${{ secrets.MINIO_SECRET_KEY }}  ${{ steps.vars.outputs.META_URL }} fio-benchmark
                ./$bin_name mount -d ${{ steps.vars.outputs.META_URL }} ${{ steps.vars.outputs.MOUNT_POINT }} --no-usage-report $mount_arg &
                if [[ "$name" =~ ^big-file-rand-read.* ]]; then
                  block_size=$(echo $name | awk -F- '{print $NF}' | xargs)
                  echo block_size is $block_size
                  fio --name=big-file-rand-read-preload --directory=${{ steps.vars.outputs.MOUNT_POINT }} --rw=randread --refill_buffers --size=${size} --filename=randread.bin --bs=$block_size --pre_read=1
                  sudo sync && echo 3 > /proc/sys/vm/drop_caches
                fi

                echo "start fio"
                fio --name=$name --directory=${{ steps.vars.outputs.MOUNT_POINT }} $fio_arg > fio/$name-$juicefs_version.log
                echo "finish fio"

                bw_str=$(tail -1 fio/$name-$juicefs_version.log | awk '{print $2}' | awk -F '=' '{print $2}' )
                bw=$(echo $bw_str | sed 's/.iB.*//g') 
                if [[ $bw_str == *KiB* ]]; then
                  bw=$(echo "scale=2; $bw/1024.0" | bc -l)
                elif [[ $bw_str == *GiB* ]]; then
                  bw=$(echo "scale=2; $bw*1024.0" | bc -l)
                fi
                bw_list+=($bw)

                echo $name, $bw, $juicefs_version, $size, $nrfiles, ${{ github.ref_name }}, $(date +"%Y-%m-%d %H:%M:%S"), ${{github.sha}}, https://github.com/${{github.repository}}/actions/runs/${{github.run_id}},  redis, minio | tee -a $csv_file
              
                ./$bin_name umount -f ${{ steps.vars.outputs.MOUNT_POINT }}
                UUID=$(./juicefs status ${{ steps.vars.outputs.META_URL }} | grep UUID | cut -d '"' -f 4)
                if [ -n "$UUID" ];then
                  sudo ./juicefs destroy --force ${{ steps.vars.outputs.META_URL }} $UUID
                fi
                mc rb  myminio/$name --force --dangerous || printf "Warining; remove bucket failed: %s, exit code: %s" "myminio/$name" "$?"
              fi
            done
            
            new=${bw_list[0]}
            old=$(expr ${bw_list[1]}*$decrease_ratio | bc)
            echo new is $new, old is $old
            if (( $(echo "$new < $old" |bc -l) )); then
              printf "Fatal: bandwidth decreased too much: new is: %s, old is: %s\n" ${bw_list[0]} ${bw_list[1]} 
              fail=true
            fi
          done
          mysql -h 172.27.0.1 -u root -p${{secrets.MYSQL_PASSWORD}} -e "use  test_result; LOAD DATA INFILE '/var/lib/mysql-files/result.csv' INTO TABLE fio_test FIELDS TERMINATED BY ',' (name, bandwidth,  juicefs_version, size, nrfiles, ref_name, created_date, github_revision, workflow_url, meta_engine, storage); " 
          if $fail; then
            echo "Fatal: fio bandwidth decreased by 20% compared with last version, please check the result file"
            exit 1
          fi

      - name: Save test reuslt
        if: ${{always()}}
        uses: actions/upload-artifact@v2
        with: 
          name: result.csv
          path: /var/lib/mysql-files/result.csv
      
      - name: Save fio logs
        if: ${{always()}}
        uses: actions/upload-artifact@v2
        with: 
          name: fio-logs
          path: fio/

      - name: Clean After
        if: ${{always()}}
        run: |
          if [ -d ${{ steps.vars.outputs.MOUNT_POINT }} ]; then
            ./juicefs umount ${{ steps.vars.outputs.MOUNT_POINT }} || true
          fi
          UUID=$(./juicefs status ${{ steps.vars.outputs.META_URL }} | grep UUID | cut -d '"' -f 4)
          if [ -n "$UUID" ];then
            sudo ./juicefs destroy --force ${{ steps.vars.outputs.META_URL }} $UUID
          fi

      - name: Send Slack Notification
        if: ${{ failure() }}
        uses: juicedata/slack-notify-action@main
        with:
          channel-id: "${{ secrets.SLACK_CHANNEL_ID_FOR_PR_CHECK_NOTIFY }}"
          slack_bot_token: "${{ secrets.SLACK_BOT_TOKEN }}"  
